{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CMPE 462 - Project 1<br>Binary Classification with Logistic Regression<br>Due: April 23, 2020, 23:59</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Student ID1: 2016400198**\n",
    "* **Student ID2: 2016400126**\n",
    "* **Student ID3: 2018400279**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this project, you are going to implement logistic regression from scratch. You are provided\n",
    "a subset of the famous handwritten digit dataset called MNIST. In the subset, you will find images of digit 1 and 5. Therefore, you will be solving a binary classification problem. The project includes feature extraction, model training, and evaluation steps.\n",
    "\n",
    "First, you will load and visualize the data portion we have provided to you and then extract two different set of features to build a classifier on. When you extracted the desired features, you will run your logistic regression implementation with gradient descent on the representations to classify digits into 1 and 5. You will experiment with different learning rates  and regularization parameter ($\\lambda$) and find the optimal $\\lambda$ with 5-fold cross validation. Finally, you will evaluate the implemented models, decide which is the best performing one and visualize a decision boundary.\n",
    "\n",
    "Follow the steps on this notebook that would guide you through the solution step-by-step. Make sure that the notebook reports your work properly and add comments and opinions when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** You are allowed to use third-party libraries such as `numpy` and `matplotlib` to help you implement necessary procedures. However, you should not import any function that accomplishes the task itself. For instance, you can use `numpy` arrays for matrix operations, but you cannot use `scikit-learn` to implement cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Feature Extraction (35 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training/test data and labels as numpy arrays (Hint:`np.load`). Train and test data are 1561x256 and 424x256 dimensional matrices, respectively. Each row in the\n",
    "aforementioned matrices corresponds to an image of a digit. The 256 pixels correspond to a 16x16 image. Label 1 is assigned to digit 1 and label -1 is assigned to digit 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"data/train_data.npy\")\n",
    "X_test = np.load(\"data/test_data.npy\")\n",
    "y_train = np.load(\"data/train_labels.npy\")\n",
    "y_test = np.load(\"data/test_labels.npy\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Display two of the digit images, one for digit 1 and one for digit 5. You can use `imshow` function of `matplotlib` library with a suitable colormap. You will first need to reshape 256 pixels to a 16x16 matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(np.reshape(X_train[0],(16,16)),cmap=\"gray\")\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(np.reshape(X_train[-1],(16,16)),cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points) Implementing Representation 1:** Extract the **symmetry** and **average intensity** features discussed in the class (see logistic regression lecture notes). To compute the intensity features, compute the average pixel value of the image, and for the symmetry feature, compute the negative of the norm of the difference between the image and its y-axis symmetrical. Search numpy's documentation to find the suitable function at each step. You should extract these two features for each image in the training and test sets. As a result, you should obtain a training data matrix of size 1561x2 and test data matrix of size 424x2.\n",
    "\n",
    "Throughout the notebook, we will refer the representation with these two features as **Representation 1** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_1_train = np.zeros((len(X_train),2)) # first index -> symmetry, second index -> intensity\n",
    "rep_1_test = np.zeros((len(X_test),2)) # first index -> symmetry, second index -> intensity\n",
    "\n",
    "# Intensity\n",
    "rep_1_train[:,1] = np.sum(X_train,axis=1)\n",
    "rep_1_test[:,1] = np.sum(X_test,axis=1)\n",
    "\n",
    "# Symmetry\n",
    "for i in range(len(X_train)):\n",
    "    x_train = np.reshape(X_train[i],(16,16))\n",
    "    x_train_flipped = np.flip(x_train,axis=1) \n",
    "    rep_1_train[i,0] = np.sum(-1*abs(x_train-x_train_flipped))\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    x_test = np.reshape(X_test[i],(16,16))\n",
    "    x_test_flipped = np.flip(x_test,axis=1)\n",
    "    rep_1_test[i,0] = np.sum(-1*abs(x_test-x_test_flipped))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Provide two scatter plots, one for training and one for test data. The plots should contain the average intensity values in the x-axis and symmetry values in the\n",
    "y-axis. Denote the data points of label 1 with blue marker shaped <font color='blue'>o</font> and the data points of label -1 with a red marker shaped <font color='red'>x</font>. (Hint: check out `plt.scatter` and its `marker` and `color` parameters). Explicitly state the axis labels and figure title for both plots (Hint: `plt.xlabel`, `plt.ylabel`, `plt.title`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.scatter(rep_1_train[y_train == 1][:,0],rep_1_train[y_train == 1][:,1],c=\"r\")\n",
    "plt.scatter(rep_1_train[y_train == -1][:,0],rep_1_train[y_train == -1][:,1],c=\"b\")\n",
    "plt.xlabel(\"symmetry value\")\n",
    "plt.ylabel(\"intensity value\")\n",
    "plt.title(\"Train Data\")\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(rep_1_test[y_test == 1][:,0],rep_1_test[y_test == 1][:,1],c=\"r\")\n",
    "plt.scatter(rep_1_test[y_test == -1][:,0],rep_1_test[y_test == -1][:,1],c=\"b\")\n",
    "plt.xlabel(\"symmetry value\")\n",
    "plt.ylabel(\"intensity value\")\n",
    "plt.title(\"Test Data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a outlier at the bottom-right of the training data plot. We want to visualize that outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = rep_1_train[y_train==-1]  # Take all data of first class\n",
    "outlier_index = np.argmax(class_0[:,0]>-50)  # Find the index of outlier\n",
    "\n",
    "for a,i in enumerate(rep_1_train[:,0]==class_0[outlier_index][0]):\n",
    "    if i:\n",
    "        plt.imshow(np.reshape(X_train[1517],(16,16)),cmap=\"gray\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This data was labeled as 5, however it doesn't seem like 5 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(15 points) Implementing Representation 2:** Come up with an alternative feature extraction approach. The features can again be 2-D, or higher dimensional. If you use any external resource, please cite the references. Explain the feature extraction procedure clearly in your report; if it is an algorithm, provide the algorithm; if it is a function, provide the mathematical expressions. \n",
    "\n",
    "If your proposed features are 2-D or 3-D, provide the scatter plots similar to the previous step.\n",
    "\n",
    "We will refer this representation proposed by you as **Representation 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We chose 2 features. \n",
    "#### First one is the variance of the x coordinates of white pixels. We expect that this feature will be low for \"1\" images and high for \"5\" images.\n",
    "#### Second one is the distance on x axis between the white pixel with maximum x coordinate value and the one with minimum x coordinate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "rep_2_train = np.zeros((len(X_train),2))\n",
    "rep_2_test = np.zeros((len(X_test),2))\n",
    "\n",
    "          \n",
    "for index, x in enumerate(X_train.reshape(-1,16,16)):\n",
    "    x_indices = []\n",
    "    indices = x > 0\n",
    "    \n",
    "    # Find white pixels and store their x coordinates.\n",
    "    for i in range(len(indices)):\n",
    "        for j in range(len(indices[i])):\n",
    "            if indices[i][j]:  \n",
    "                x_indices.append(j)\n",
    "\n",
    "    rep_2_train[index,0] = np.var(x_indices)\n",
    "    rep_2_train[index,1] = max(x_indices) - min(x_indices)\n",
    "\n",
    "\n",
    "for index, x in enumerate(X_test.reshape(-1,16,16)):\n",
    "    x_indices = []\n",
    "    indices = x > 0\n",
    "\n",
    "    # Find white pixels and store their x coordinates.\n",
    "    for i in range(len(indices)):\n",
    "        for j in range(len(indices[i])):\n",
    "            if indices[i][j]:\n",
    "                x_indices.append(j)\n",
    "\n",
    "    rep_2_test[index,0] = np.var(x_indices)\n",
    "    rep_2_test[index,1] = max(x_indices) - min(x_indices)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.scatter(rep_2_train[y_train == 1][:,0],rep_2_train[y_train == 1][:,1],c=\"r\")\n",
    "plt.scatter(rep_2_train[y_train == -1][:,0],rep_2_train[y_train == -1][:,1],c=\"b\")\n",
    "plt.xlabel(\"variance value\")\n",
    "plt.ylabel(\"difference of x coordinates\")\n",
    "plt.title(\"Train Data\")\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(rep_2_test[y_test == 1][:,0],rep_2_test[y_test == 1][:,1],c=\"r\")\n",
    "plt.scatter(rep_2_test[y_test == -1][:,0],rep_2_test[y_test == -1][:,1],c=\"b\")\n",
    "plt.xlabel(\"variance value\")\n",
    "plt.ylabel(\"difference of x coordinates\")\n",
    "plt.title(\"Test Data\")\n",
    "\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Logistic Regression (40 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(20 points)** Implement the logistic regression classifier from scratch with gradient descent and train it using Representation 1 and Representation 2 as inputs. Concatenate\n",
    "1 to your features for the intercept term, such that one data point will look like for 2-D features [1,$x_1$,$x_2$], and the model vector will be [$w_0, w_1, w_2$], where $w_0$ is the intercept parameter. \n",
    "You can refer to lecture notes (Logistic regression slides 29-30) to review the gradient descent learning algorithm and the logistic loss. To implement the gradient of the logistic loss with respect to $w$, first derive its expression by hand. Please include your derivation in your report.\n",
    "\n",
    "To prove that your implementation is converging, keep the loss values at each gradient descent iteration in a numpy array. After the training is finalized, plot the loss values\n",
    "with respect to iteration count (Hint: `plt.plot`). You should observe a decreasing loss as the number of iterations increases. Also, experiment with 5 different learning rates between 0 and 1, and plot the convergence curves for each learning rate in the same plot to observe the effect of the learning rate (step size) on the convergence. \n",
    "\n",
    "To decide when to terminate the gradient descent iterations, check the absolute difference between the current loss value and the loss value of the previous step. If the difference is less than a small number, such as $10^{-5}$, you can exit the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = np.concatenate((np.ones((len(rep_1_train),1)), rep_1_train), axis=1)\n",
    "X_test_1 = np.concatenate((np.ones((len(rep_1_test),1)), rep_1_test), axis=1)\n",
    "\n",
    "X_train_2 = np.concatenate((np.ones((len(rep_2_train),1)), rep_2_train), axis=1)\n",
    "X_test_2 = np.concatenate((np.ones((len(rep_2_test),1)), rep_2_test), axis=1)\n",
    "\n",
    "print(\"Data Summary\")\n",
    "print(\"1st Representation -> X-train = {}, y-train = {}, X-test = {}, y-test = {}\".format(str(X_train_1.shape), str(y_train.shape), str(X_test_1.shape), str(y_test.shape)))\n",
    "print(\"2nd Representation -> X-train = {}, y-train = {}, X-test = {}, y-test = {}\".format(str(X_train_2.shape), str(y_train.shape), str(X_test_2.shape), str(y_test.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logistic_loss(y_pred, y_true, reg_lambda=None, w=None):\n",
    "    if reg_lambda:\n",
    "        return np.sum(np.log(1 + np.exp(np.multiply(y_pred, -y_true))))/len(y_true) + reg_lambda*np.sum(w.dot(w))\n",
    "    else:\n",
    "        return np.sum(np.log(1 + np.exp(np.multiply(y_pred, -y_true))))/len(y_true)\n",
    "\n",
    "\n",
    "# Fits logistic regression model to given data. \n",
    "# Returns weight matrix and loss history.\n",
    "def fit_logistic_model(X, y, epoch, learning_rate, reg_lambda=None):\n",
    "    \n",
    "    w = np.zeros((len(X[0])))\n",
    "    loss_history = []\n",
    "\n",
    "    for step in range(epoch):\n",
    "\n",
    "        y_pred = X.dot(w.T)\n",
    "\n",
    "        if reg_lambda:\n",
    "            loss = logistic_loss(y_pred, y, reg_lambda, w)\n",
    "        else:\n",
    "            loss = logistic_loss(y_pred, y)\n",
    "\n",
    "        grad_1 = (1 / (1 + np.exp(np.multiply(y, X.dot(w.T)))))\n",
    "        grad_2 = (-1 * (np.reshape(y, (-1,1)) * X))\n",
    "        grad = np.reshape(grad_1, (-1,1)) * grad_2\n",
    "        grad = np.sum(grad,axis=0)/len(y)\n",
    "\n",
    "        if reg_lambda:\n",
    "            grad = grad + 2*reg_lambda*w\n",
    "\n",
    "        w[:] = w - learning_rate*grad\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Early Stopping check after 50th epoch\n",
    "        if len(loss_history) > 50 and loss_history[-2]-loss_history[-1]<1e-5:\n",
    "            break\n",
    "    \n",
    "    return [w, loss_history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Rep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,5))\n",
    "\n",
    "lr_list = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    fig.add_subplot(1,5,i+1)\n",
    "    w, losses = fit_logistic_model(X_train_1, y_train, epoch=5000, learning_rate=lr)\n",
    "    plt.scatter(range(len(losses)), losses)\n",
    "    plt.title(\"LR = \"+str(lr)+\" Loss = \"+str(losses[-1])[:6])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Rep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,5))\n",
    "\n",
    "# Larger lr worked better for this case\n",
    "lr_list = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    fig.add_subplot(1,5,i+1)\n",
    "    w, losses = fit_logistic_model(X_train_2, y_train, epoch=5000, learning_rate=lr)\n",
    "    plt.scatter(range(len(losses)), losses)\n",
    "    plt.title(\"LR = \"+str(lr)+\" Loss = \"+str(losses[-1])[:6])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Implement logistic regression with $\\ell_2$ norm regularization, $||\\mathbf{w}||_{2}^{2}$ . Show that your implementation is working by visualizing the loss over the iterations again. Visualization for a single learning rate and $\\lambda$ suffices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data was set and train function was already written above.\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "# Rep 1\n",
    "fig.add_subplot(1,2,1)\n",
    "lr = 1e-3\n",
    "w, losses = fit_logistic_model(X_train_1, y_train, epoch=5000, learning_rate=lr, reg_lambda=0.001)\n",
    "plt.scatter(range(len(losses)), losses)\n",
    "plt.title(\"Rep 1 - LR = \"+str(lr)+\", Loss = \"+str(losses[-1])[:6] + \", Reg = 0.001\")\n",
    "\n",
    "# Rep 2\n",
    "fig.add_subplot(1,2,2)\n",
    "lr = 1e-3\n",
    "w, losses = fit_logistic_model(X_train_2, y_train, epoch=5000, learning_rate=lr, reg_lambda=0.001)\n",
    "plt.scatter(range(len(losses)), losses)\n",
    "plt.title(\"Rep 2 - LR = \"+str(lr)+\", Loss = \"+str(losses[-1])[:6] + \", Reg = 0.001\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Implement a 5-fold cross validation procedure to find the optimal $\\lambda$ value for both Representation 1 and 2. Experiment with at least three different $\\lambda$ values between 0 and 1. Report the mean/std of cross validation accuracy of every representation/parameter combination as a table and clearly mark the best configuration in your report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns accuracy of predictions made by given weigths.\n",
    "def accuracy(X, y, w):\n",
    "    correct = np.sum(np.logical_not(np.logical_xor(sigmoid(X.dot(w))>0.5, y==1)))\n",
    "    wrong = len(y) - correct\n",
    "    return correct/len(y)\n",
    "\n",
    "\n",
    "# Merges given lists except the one at \"index\"\n",
    "def merger(input_list, index):\n",
    "    result = []\n",
    "    for i in range(len(input_list)):\n",
    "        if i is not index:\n",
    "            result.extend(input_list[i])\n",
    "    return result\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV for 1st Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the train data for 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data = []\n",
    "splitted_truth = []\n",
    "\n",
    "for i in range(5):\n",
    "    splitted_data.append([])\n",
    "    splitted_truth.append([])\n",
    "\n",
    "# For randomness, we split the data with the following way using mod\n",
    "for i in range(X_train_1.shape[0]):\n",
    "    splitted_data[i%5].append(X_train_1[i])\n",
    "    splitted_truth[i%5].append(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "training_result_list = []\n",
    "lambdas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    acc_hist = []\n",
    "    for j in range(5):\n",
    "        validation_list = splitted_data[j]\n",
    "        validation_truth = splitted_truth[j]\n",
    "        folded_rep_1_training_data = merger(splitted_data,j)\n",
    "        folded_rep_1_truth = merger(splitted_truth,j)\n",
    "        training_result = fit_logistic_model(np.array(folded_rep_1_training_data), np.array(folded_rep_1_truth), epoch=5000, learning_rate=1e-3, reg_lambda=lambdas[i])\n",
    "        acc_hist.append(accuracy(np.array(validation_list), np.array(validation_truth), training_result[0]))\n",
    "        training_result_list.append(training_result)\n",
    "    \n",
    "    means.append(np.mean(acc_hist))\n",
    "    stds.append(np.std(acc_hist))\n",
    "\n",
    "print(\"LR = 1e-3\")\n",
    "for i in range(len(lambdas)):\n",
    "    print(\"Rep. 1 -> Lambda = {}, Avg Accuracy = {}, Std of Accuracy = {}\".format(lambdas[i], means[i], stds[i]))\n",
    "\n",
    "print(\"################################################################################################\")\n",
    "best_lambda_i = np.argmax(means)\n",
    "print(\"Rep. 1 -> Best Lambda = {}, Best Accuracy = {}\".format(lambdas[best_lambda_i], means[best_lambda_i]))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV for 2nd Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the train data for 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data = []\n",
    "splitted_truth = []\n",
    "\n",
    "for i in range(5):\n",
    "    splitted_data.append([])\n",
    "    splitted_truth.append([])\n",
    "\n",
    "# For randomness, we split the data with the following way using mod\n",
    "for i in range(X_train_2.shape[0]):\n",
    "    splitted_data[i%5].append(X_train_2[i])\n",
    "    splitted_truth[i%5].append(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "training_result_list = []\n",
    "lambdas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    acc_hist = []\n",
    "    for j in range(5):\n",
    "        validation_list = splitted_data[j]\n",
    "        validation_truth = splitted_truth[j]\n",
    "        folded_rep_2_training_data = merger(splitted_data,j)\n",
    "        folded_rep_2_truth = merger(splitted_truth,j)\n",
    "        training_result = fit_logistic_model(np.array(folded_rep_2_training_data), np.array(folded_rep_2_truth), epoch=5000, learning_rate=1e-3, reg_lambda=lambdas[i])\n",
    "        acc_hist.append(accuracy(np.array(validation_list), np.array(validation_truth), training_result[0]))\n",
    "        training_result_list.append(training_result)\n",
    "    \n",
    "    means.append(np.mean(acc_hist))\n",
    "    stds.append(np.std(acc_hist))\n",
    "\n",
    "print(\"LR = 1e-3\")\n",
    "for i in range(len(lambdas)):\n",
    "    print(\"Rep. 2 -> Lambda = {}, Avg Accuracy = {}, Std of Accuracy = {}\".format(lambdas[i], means[i], stds[i]))\n",
    "\n",
    "print(\"################################################################################################\")\n",
    "best_lambda_i = np.argmax(means)\n",
    "print(\"Rep. 2 -> Best Lambda = {}, Best Accuracy = {}\".format(lambdas[best_lambda_i], means[best_lambda_i]))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Evaluation (25 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Train the logistic regression classifier on Representation 1 and 2 with the best learning rate you decide. Similarly, train the regularized logistic regression classifier with the best $\\lambda$ you obtained by 5-fold cross validation. Report the training and test classification accuracy as \n",
    "\\begin{align*}\n",
    "\\frac{\\text{number of correctly classified samples}}{\\text{total number of samples}}x100\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best parameters for rep. 1 and Rep. 2:\n",
    "### LR = 1e-3, Reg = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1_no_reg, _ = fit_logistic_model(X_train_1, y_train, 5000, learning_rate=1e-3, reg_lambda=None)\n",
    "w_2_no_reg, _ = fit_logistic_model(X_train_2, y_train, 5000, learning_rate=1e-3, reg_lambda=None)\n",
    "\n",
    "w_1_reg, _ = fit_logistic_model(X_train_1, y_train, 5000, learning_rate=1e-3, reg_lambda=0.1)\n",
    "w_2_reg, _ = fit_logistic_model(X_train_2, y_train, 5000, learning_rate=1e-3, reg_lambda=0.1)\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(\"Rep. 1 Accuracy: \", accuracy(X_train_1, y_train, w_1_no_reg)*100)\n",
    "print(\"Rep. 1 Accuracy (Reg): \", accuracy(X_train_1, y_train, w_1_reg)*100)\n",
    "print(\"Rep. 2 Accuracy: \", accuracy(X_train_2, y_train, w_2_no_reg)*100)\n",
    "print(\"Rep. 2 Accuracy (Reg): \", accuracy(X_train_2, y_train, w_2_reg)*100)\n",
    "\n",
    "print(\"\\nTEST\")\n",
    "print(\"Rep. 1 Accuracy: \", accuracy(X_test_1, y_test, w_1_no_reg)*100)\n",
    "print(\"Rep. 1 Accuracy (Reg): \", accuracy(X_test_1, y_test, w_1_reg)*100)\n",
    "print(\"Rep. 2 Accuracy: \", accuracy(X_test_2, y_test, w_2_no_reg)*100)\n",
    "print(\"Rep. 2 Accuracy (Reg): \", accuracy(X_test_2, y_test, w_2_reg)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(15 points)** Visualize the decision boundary (the line that is given by $\\mathbf{w}^{T}x=0$) obtained from the logistic regression classifier learned without regularization. For this purpose, use only Representation 1. Provide two scatter plots for training and test data points with the decision boundary shown on each of the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-250,0,10)\n",
    "y = x*(-w_1_no_reg[1]/w_1_no_reg[2]) + (-w_1_no_reg[0]/w_1_no_reg[2])\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.scatter(rep_1_train[y_train == 1][:,0],rep_1_train[y_train == 1][:,1])\n",
    "plt.scatter(rep_1_train[y_train == -1][:,0],rep_1_train[y_train == -1][:,1])\n",
    "plt.plot(x, y, \"r--\")\n",
    "plt.xlabel(\"symmetry value\")\n",
    "plt.ylabel(\"intensity value\")\n",
    "plt.title(\"Train Data\")\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(rep_1_test[y_test == 1][:,0],rep_1_test[y_test == 1][:,1])\n",
    "plt.scatter(rep_1_test[y_test == -1][:,0],rep_1_test[y_test == -1][:,1])\n",
    "plt.plot(x, y, \"r--\")\n",
    "plt.xlabel(\"symmetry value\")\n",
    "plt.ylabel(\"intensity value\")\n",
    "plt.title(\"Test Data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Comment on your work in your report. Include the answers for the following questions in your discussion. \n",
    "\n",
    "* Did regularization improve the generalization performance (did it help reducing the gap between training and test accuracies/errors)? Did you observe any difference between using Representation 1 and 2?\n",
    "* Which feature set did give the best results? Which one is more discriminative?\n",
    "* What would be your next step to improve test accuracy? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}