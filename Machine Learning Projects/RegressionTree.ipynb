{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CMPE 462 - Quiz 4&5 <br>Implementing a Regression Tree<br>Due: May 3, 2020, 23:59</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Decision trees can learn rules to map features to both continous and discrete outputs. In this quiz, you will consider the continous case and implement a regression tree to predict house prices in Boston. You will also conduct small data analysis and evaluation procedures. This notebook will guide you through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 pts) Task 1: Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to load Boston house prices dataset using `scikit-learn`. You can find more detail on the dataset [here](https://scikit-learn.org/stable/datasets/index.html#boston-dataset).\n",
    "\n",
    "When the dataset is loaded, construct train and test matrices, by allocating **first 400 samples** to train and the rest to test. ___Do not shuffle or randomly sample the feature matrix___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X_y = np.column_stack([boston['data'], boston['target']])\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(X_y)\n",
    "X, y = X_y[:,:-1], X_y[:,-1]\n",
    "X_train, y_train = X[:400], y[:400]\n",
    "X_test, y_test = X[400:], y[400:]\n",
    "del X, y, X_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.keys()) # Keys of bunch\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Data:\", \"X:\", X_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Test Data:\", \"X:\", X_test.shape, \"y:\", y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 pts)** Unlike the dataset in Project 1, Boston dataset is high-dimensional and we cannot observe the relations between all features and the output with a single scatter plot. What we can do is to visualize the correlations between both features and house prices using a heatmap. So, stack the features and the outputs in a single matrix and compute correlation between all pairs of columns. Visualize the correlation coefficient matrix as a heatmap, which is $(N+1)x(N+1)$, where $N$ is the number of features in Boston dataset. You can check out `corrcoef` and `heatmap` functions from `numpy` and `seaborn` libraries, respectively. You can use diverging color palette to emphasize both positive and negative correlations.\n",
    "\n",
    "Do you observe strong correlations between any pair of features or certain features and house price? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_mat = np.concatenate([X_train.transpose(), y_train.reshape(1,-1)])\n",
    "corr = np.corrcoef(concat_mat.reshape(-1,400))\n",
    "names = list(boston.feature_names) + [\"y\"] \n",
    "sns.heatmap(corr, xticklabels=names, yticklabels=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, all features and target have maximum correlation with itself, as expected. In addition, according to the heatmap above, target has high correlation with the feature \"LSTAT\". \"RAD\" and \"TAX\" also have a very strong correlation. For other features, strong correlations can be obtained as in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Strong Correlations\")\n",
    "for i in range(len(names)):\n",
    "    second_largest_i = np.argsort(abs(corr)[i,:])[-2]\n",
    "    print(names[i], \"has the strongest correlation with\", names[second_largest_i]+ \"(\"+str(second_largest_i)+\")\", \" - Corr:\", corr[i,:][second_largest_i])\n",
    "\n",
    "print(\"\\nWeak Correlations\")\n",
    "for i in range(len(names)):\n",
    "    second_largest_i = np.argsort(abs(corr)[i,:])[0]\n",
    "    print(names[i], \"has the weakest correlation with\", names[second_largest_i]+ \"(\"+str(second_largest_i)+\")\", \" - Corr:\", corr[i,:][second_largest_i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (80 pts) Task 2: Regression Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(15 pts)** Let us now build the regression tree. We start by implementing the splitting criteria which is used to decide the most discriminative features at each step. We stick to lecture notes and use $RSS$ which is computed as follows:\n",
    "\n",
    "$$RSS =\\sum_{\\text {left }}\\left(y_{i}-y_{L}^{*}\\right)^{2}+\\sum_{\\text {right }}\\left(y_{i}-y_{R}^{*}\\right)^{2}$$\n",
    "\n",
    "where $y_L^* and y_L^*$ are mean y-value of left and right nodes.\n",
    "\n",
    "When you implement $RSS$, pick the most correlated and least correlated feature with the output prices according to previous step. Note that correlation can be both positive and negative! For both features, compute $RSS$ for every possible split threshold and plot thresholds versus RSS scores.\n",
    "\n",
    "Do two features display different characteristics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y has the strongest correlation with LSTAT(12)  - Corr: -0.7353565653971893\n",
    "#### y has the weakest correlation with CHAS(3)  - Corr: 0.1964473680907214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss(X_i, y, threshold):\n",
    "    y_left = y[X_i<threshold]\n",
    "    y_right = y[X_i>=threshold]\n",
    "    y_left_mean = np.mean(y_left)\n",
    "    y_right_mean = np.mean(y_right)\n",
    "    return np.sum(np.power((y_left - y_left_mean),2)) + np.sum(np.power((y_right - y_right_mean),2)) \n",
    "\n",
    "def find_best_threshold(X_i, y):\n",
    "    thresholds = []\n",
    "    rss_values = []\n",
    "    sorted_X_i = sorted(X_i)\n",
    "    for i in range(len(X_i)-1):\n",
    "        split_threshold = (sorted_X_i[i]+sorted_X_i[i+1])/2\n",
    "        current_rss = rss(X_i, y, split_threshold)\n",
    "        thresholds.append(split_threshold)\n",
    "        rss_values.append(current_rss)\n",
    "    \n",
    "    return thresholds[np.argmin(rss_values)], thresholds, rss_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTAT = X_train[:,12]\n",
    "CHAS = X_train[:,3]\n",
    "\n",
    "LSTAT_threshold, LSTAT_thresholds, LSTAT_rss_values = find_best_threshold(LSTAT, y_train)\n",
    "CHAS_threshold, CHAS_thresholds, CHAS_rss_values = find_best_threshold(CHAS, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best threshold for LSTAT:\", LSTAT_threshold, \"RSS:\")\n",
    "sns.scatterplot(LSTAT_thresholds, LSTAT_rss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best threshold for CHAS:\", CHAS_threshold)\n",
    "sns.scatterplot(CHAS_thresholds, CHAS_rss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best threshold for LSTAT gives a better rss value than the one for CHAS (~18k < ~30k). That means, LSTAT gives more information when used as decision feature with selected threshold as expected, because it has stronger correlation with the target. In addition, we can say that CHAS is a binary variable, because there are only 3 threshold that we can choose which are 0, 0.5 and 1, as we can see from the graph above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(45 pts)** Now implement the training procedure of a regression tree using $RSS$ as split criteria. Build a rule tree recursively by traversing all features and considering each split threshold to find the optimum split, at every node.\n",
    "\n",
    "You are free to implement training procedure as a standalone function or part of a class, but in any case use maximum depth as the stopping condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree:\n",
    "    def __init__(self, depth=0):\n",
    "        self.depth = depth\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.prediction = None # If not a leaf, this is None.\n",
    "        self.left_node = None # Another RegressionTree\n",
    "        self.right_node = None # Another RegressionTree\n",
    "\n",
    "\n",
    "def fit_tree(tree, X, y, max_depth, prev_mean=None):\n",
    "\n",
    "    if max_depth < 1:\n",
    "        AssertionError(\"max_depth must be higher than 0\")\n",
    "\n",
    "    if tree.depth == max_depth or len(y) == 1:\n",
    "        tree.prediction = prev_mean\n",
    "        return\n",
    "\n",
    "    min_rss = None\n",
    "    best_feature_i = None\n",
    "    best_threshold = None\n",
    "\n",
    "    for feature_i in range(X.shape[1]):\n",
    "        X_i = X[:,feature_i]\n",
    "        threshold, _, rss_values = find_best_threshold(X_i, y)\n",
    "        if min_rss is None or min_rss > np.min(rss_values):\n",
    "            min_rss = np.min(rss_values)\n",
    "            best_feature_i = feature_i\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    tree.feature_index = best_feature_i\n",
    "    tree.threshold = best_threshold\n",
    "    tree.left_node = RegressionTree(tree.depth+1)\n",
    "    tree.right_node = RegressionTree(tree.depth+1)\n",
    "\n",
    "    left_X = X[X[:, tree.feature_index] < tree.threshold] \n",
    "    left_y = y[X[:, tree.feature_index] < tree.threshold]\n",
    "    left_prev_mean = np.mean(left_y)\n",
    "\n",
    "    right_X = X[X[:, tree.feature_index] >= tree.threshold] \n",
    "    right_y = y[X[:, tree.feature_index] >= tree.threshold]\n",
    "    right_prev_mean = np.mean(right_y)\n",
    "\n",
    "    fit_tree(tree.left_node, left_X, left_y, max_depth, left_prev_mean)\n",
    "    fit_tree(tree.right_node, right_X, right_y, max_depth, right_prev_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(15 pts)** Having implemented the regression tree, now write a procedure to predict test features. Given a sample, this procedure should follow the rules learned during the training to arrive at a leaf and predict the output as the mean output of the arrived leaf samples. \n",
    "\n",
    "If you have implemented a regression tree class, you can insert this procedure as a class function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree(tree, X):\n",
    "    # X should have the shape of (N,) where N is the number of features.\n",
    "\n",
    "    if tree.prediction:\n",
    "        return tree.prediction\n",
    "\n",
    "    if X[tree.feature_index] < tree.threshold:\n",
    "        return predict_tree(tree.left_node, X)\n",
    "    else:\n",
    "        return predict_tree(tree.right_node, X) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Train your model using a max depth of 3 and visualize the resulting tree. You can use an external tool such as draw.io or LaTeX for drawing. Annotate the nodes with split columns and thresholds. You can view the tree in this [link](https://scikit-learn.org/stable/modules/tree.html#tree) as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = RegressionTree()\n",
    "fit_tree(tree, X_train, y_train, max_depth=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1st node:\", names[tree.feature_index], tree.threshold, tree.prediction) \n",
    "\n",
    "print(\"2nd node:\", names[tree.left_node.feature_index], tree.left_node.threshold, tree.left_node.prediction) \n",
    "print(\"3rd node:\", names[tree.right_node.feature_index], tree.right_node.threshold, tree.right_node.prediction) \n",
    "\n",
    "print(\"4th node:\", names[tree.left_node.left_node.feature_index], tree.left_node.left_node.threshold, tree.left_node.left_node.prediction) \n",
    "print(\"5th node:\", names[tree.left_node.right_node.feature_index], tree.left_node.right_node.threshold, tree.left_node.right_node.prediction) \n",
    "print(\"6th node:\", names[tree.right_node.left_node.feature_index], tree.right_node.left_node.threshold, tree.right_node.left_node.prediction) \n",
    "print(\"7th node:\", names[tree.right_node.right_node.feature_index], tree.right_node.right_node.threshold, tree.right_node.right_node.prediction)\n",
    "\n",
    "print(\"8th node:\", tree.left_node.left_node.left_node.feature_index, tree.left_node.left_node.left_node.threshold, tree.left_node.left_node.left_node.prediction) \n",
    "print(\"9th node:\", tree.left_node.left_node.right_node.feature_index, tree.left_node.left_node.right_node.threshold, tree.left_node.left_node.right_node.prediction) \n",
    "print(\"10th node:\", tree.left_node.right_node.left_node.feature_index, tree.left_node.right_node.left_node.threshold, tree.left_node.right_node.left_node.prediction) \n",
    "print(\"11th node:\", tree.left_node.right_node.right_node.feature_index, tree.left_node.right_node.right_node.threshold, tree.left_node.right_node.right_node.prediction)\n",
    "print(\"12th node:\", tree.right_node.left_node.left_node.feature_index, tree.right_node.left_node.left_node.threshold, tree.right_node.left_node.left_node.prediction) \n",
    "print(\"13th node:\", tree.right_node.left_node.right_node.feature_index, tree.right_node.left_node.right_node.threshold, tree.right_node.left_node.right_node.prediction) \n",
    "print(\"14th node:\", tree.right_node.right_node.left_node.feature_index, tree.right_node.right_node.left_node.threshold, tree.right_node.right_node.left_node.prediction) \n",
    "print(\"15th node:\", tree.right_node.right_node.right_node.feature_index, tree.right_node.right_node.right_node.threshold, tree.right_node.right_node.right_node.prediction) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see from the image, RM and LSTAT were used as desicion at the top of the tree. It is expected, because they have a good correlation with y.\n",
    "<img src=\"tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(10 pts)** Task 3: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Now is time to pick the best maximum depth and observe your tree's performance! Implement a 5-fold cross validation procedure to experiment with maximum depths from 3 to 10. Report mean and standard deviation for each depth and pick the best one. For comparison you can use $R^2$, which is a metric frequently used to evaluate regression models. You can use `r2_score` function of `scikit-learn` and read more [here](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def get_r2_score(tree, X, y):\n",
    "    preds = []\n",
    "    for i in range(len(X)):\n",
    "        preds.append(predict_tree(tree, X[i]))\n",
    "    return r2_score(y, preds)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(3,11))\n",
    "scores = []\n",
    "\n",
    "for depth_i, max_depth in enumerate(depths):\n",
    "    \n",
    "    tree = RegressionTree()\n",
    "    split_lenght = len(X_train)//5 \n",
    "    split_indices = [split_lenght*i for i in range(5)]\n",
    "    scores.append([])\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        val_indices = np.zeros(X_train.shape[0], dtype=\"bool\")\n",
    "        val_indices[split_indices[i]:split_indices[i]+split_lenght] = True\n",
    "\n",
    "        X_val_split = X_train[val_indices,:]\n",
    "        X_train_split = X_train[np.logical_not(val_indices),:]\n",
    "        y_val_split = y_train[val_indices]\n",
    "        y_train_split = y_train[np.logical_not(val_indices)]\n",
    "\n",
    "        fit_tree(tree, X_train_split, y_train_split, max_depth)\n",
    "        scores[depth_i].append(get_r2_score(tree, X_val_split, y_val_split))\n",
    "    \n",
    "    print(\"Max Depth: {} - Mean R2={}, Std R2={}\".format(max_depth, np.mean(scores[depth_i]), np.sqrt(np.var(scores[depth_i]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** To conclude, train your tree one last time on the whole training data with the depth you picked in the previous section. Generate predictions on both training and test sets and report $R^2$ scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose 6 as max depth, since it gives the highest mean R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = RegressionTree()\n",
    "fit_tree(tree, X_train, y_train, max_depth=6)\n",
    "\n",
    "print(\"R2 score for Train Set:\", get_r2_score(tree, X_train, y_train))\n",
    "\n",
    "print(\"R2 score for Test Set\", get_r2_score(tree, X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit0f7e80dda11f4f039699ac39dd8cd0ae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}